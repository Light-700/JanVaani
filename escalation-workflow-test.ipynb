{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nfrom huggingface_hub import InferenceClient\nimport json\nimport requests\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T06:09:17.184699Z","iopub.execute_input":"2025-01-12T06:09:17.185103Z","iopub.status.idle":"2025-01-12T06:09:17.710040Z","shell.execute_reply.started":"2025-01-12T06:09:17.185070Z","shell.execute_reply":"2025-01-12T06:09:17.708690Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"API_KEY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T06:09:17.711794Z","iopub.execute_input":"2025-01-12T06:09:17.712185Z","iopub.status.idle":"2025-01-12T06:09:17.898882Z","shell.execute_reply.started":"2025-01-12T06:09:17.712149Z","shell.execute_reply":"2025-01-12T06:09:17.897975Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import requests\nimport time\n\n# Hugging Face API URL for the Flan-T5 model\nAPI_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-large\"\n\nheaders = {\n    'Authorization': f'Bearer {secret_value_0}',\n    'Content-Type': 'application/json'\n}\n\ndef classify_issue_priority(issue_description, retries=5, wait_time=10):\n    \"\"\"\n    Classify the priority of a civic issue using Flan-T5.\n    Returns: 'High Priority', 'Medium Priority', or 'Low Priority'\n    \"\"\"\n\n    # Creating a prompt for the Flan-T5 model\n    prompt = f\"Classify the following civic issue into High, Medium, or Low priority:\\n\\n\\\"{issue_description}\\\"\"\n\n    # Payload for the API request\n    payload = {\"inputs\": prompt}\n\n    for attempt in range(retries):\n        try:\n            response = requests.post(API_URL, headers=headers, json=payload)\n\n            if response.status_code == 200:\n                result = response.json()\n                print(\"Raw Response:\", result)  # Debugging\n\n                # Extract the output text from the model response\n                if isinstance(result, list) and 'generated_text' in result[0]:\n                    classification = result[0]['generated_text'].strip().lower()\n                    \n                    # Standardize the output\n                    if 'high' in classification:\n                        return \"High Priority\"\n                    elif 'medium' in classification:\n                        return \"Medium Priority\"\n                    elif 'low' in classification:\n                        return \"Low Priority\"\n                    else:\n                        return \"Unexpected classification: \" + classification\n\n                else:\n                    print(\"Unexpected response format:\", result)\n                    return \"Unexpected response format\"\n\n            elif response.status_code == 503:\n                print(f\"Model is loading. Retrying in {wait_time} seconds... (Attempt {attempt + 1})\")\n                time.sleep(wait_time)\n            else:\n                print(f\"Error: {response.status_code} - {response.text}\")\n                return None\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Request failed: {e}\")\n            if attempt < retries - 1:\n                print(f\"Retrying in {wait_time} seconds... (Attempt {attempt + 1})\")\n                time.sleep(wait_time)\n            else:\n                print(\"Max retries reached.\")\n                return None\n\n    print(\"Model failed to load after several attempts.\")\n    return None\n\n# ðŸš€ Test the function with a sample issue\nsample_issue = \"There is a large pothole on the main road causing traffic jams.\"\nresult = classify_issue_priority(sample_issue)\nprint(\"Predicted Priority:\", result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T06:11:22.175130Z","iopub.execute_input":"2025-01-12T06:11:22.175480Z","iopub.status.idle":"2025-01-12T06:11:22.743174Z","shell.execute_reply.started":"2025-01-12T06:11:22.175451Z","shell.execute_reply":"2025-01-12T06:11:22.742083Z"}},"outputs":[{"name":"stdout","text":"Raw Response: [{'generated_text': 'High'}]\nPredicted Priority: High Priority\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}